{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepQLearner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1SHs6riAj1s",
        "colab_type": "text"
      },
      "source": [
        "# Clonamos el repositorio para obtener los dataSet\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WFfFZvPApeD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ed07a9db-d47a-447d-d6f6-42e36d1bb2c6"
      },
      "source": [
        "!git clone https://github.com/joanby/ia-course.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ia-course'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (48/48), done.\u001b[K\n",
            "remote: Total 436 (delta 28), reused 39 (delta 13), pack-reused 375\u001b[K\n",
            "Receiving objects: 100% (436/436), 36.42 MiB | 18.11 MiB/s, done.\n",
            "Resolving deltas: 100% (95/95), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZZLJUFyA6hs",
        "colab_type": "text"
      },
      "source": [
        "# Damos acceso a nuestro Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMVaYKVrA92v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8CYKTxkJB53",
        "colab_type": "text"
      },
      "source": [
        "# Test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COqkUYzLJBEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls '/content/drive/My Drive' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eas_Lb17J4jY",
        "colab_type": "text"
      },
      "source": [
        "#Google colab tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-Kmgf5ZJ5QV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "import glob # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "from google.colab import drive # Montar tu Google drive"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odNaDE1zyrL2",
        "colab_type": "text"
      },
      "source": [
        "# Instalar dependencias de Renderizado, tarda alrededor de 45 segundos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDPDMlDo4RqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install python-opengl -y > /dev/null 2>&1\n",
        "!apt install xvfb -y --fix-missing > /dev/null 2>&1\n",
        "!apt-get install ffmpeg > /dev/null 2>&1\n",
        "!apt-get install x11-utils > /dev/null 2>&1\n",
        "!apt-get install pyglet > /dev/null 2>&1\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xxsEvp14U-Q",
        "colab_type": "text"
      },
      "source": [
        "# Instalar OpenAi Gym"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCuDUcBL4SCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install piglet > /dev/null 2>&1\n",
        "!pip install 'gym[box2d]' > /dev/null 2>&1\n",
        "#por si quieres algun environment en concreto\n",
        "#!pip install atari_py > /dev/null 2>&1\n",
        "#!pip install gym[atari] > /dev/null 2>&1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkkUdIpJ5lHQ",
        "colab_type": "text"
      },
      "source": [
        "# Todos los imports necesarios en google colab y helpers para poder visualizar OpenAi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbgBYNVD5i6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) #error only\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5DvRsCZ4b-G",
        "colab_type": "text"
      },
      "source": [
        "# Activamos una vista, seria como crear un plot de una grafica en python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDMnJvk-4cec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab9757b4-93f3-4fac-9c0c-14e3c49b7fcb"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900)) #Puedes modificar el high and width de la pantalla\n",
        "display.start()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f691d58a9e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x80iaLF94eOh",
        "colab_type": "text"
      },
      "source": [
        "# Este código crea una pantalla virtual para dibujar imágenes del juego. \n",
        "## Si se ejecuta localmente, simplemente ignóralo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0H-whKQ4fvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "if type(os.environ.get('DISPLAY')) is not str or \\\n",
        "        len(os.environ.get('DISPLAY')) == 0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMqg9TAA4qxy",
        "colab_type": "text"
      },
      "source": [
        "# Funciones de utilidad para permitir la grabación de video del ambiente del gimnasio y su visualización\n",
        "## Para habilitar la visualizacion por pantalla , tan solo haz \"**environment = wrap_env(environment)**\", por ejemplo: **environment = wrap_env(gym.make(\"MountainCar-v0\"))**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKFIMV_l4m2x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import glob\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pathlib import Path\n",
        "\n",
        "def show_one_video():\n",
        "    mp4list = glob.glob('video/*.mp4')\n",
        "    if len(mp4list) > 0:\n",
        "        mp4 = mp4list[0]\n",
        "        video = io.open(mp4, 'r+b').read()\n",
        "        encoded = base64.b64encode(video)\n",
        "\n",
        "        content = ipythondisplay.display(HTML(data='''\n",
        "        <video alt=\"test\" autoplay loop controls style=\"height: 400px;\">\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "        </video>\n",
        "        '''.format(encoded.decode('ascii'))))\n",
        "    else: \n",
        "        print(\"Couldn't find video\")\n",
        "\n",
        "def show_videos(video_path='video', prefix=''):\n",
        "  html = []\n",
        "  for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))\n",
        "\n",
        "def wrap_env(env):\n",
        "    env = gym.wrappers.Monitor(env, './video', force=True)\n",
        "    return env"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpaK1CCiVVBR",
        "colab_type": "text"
      },
      "source": [
        "# Instalar pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udH-oiduVdI6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "1439f209-d356-4ba6-8b73-76549119e9c7"
      },
      "source": [
        "!pip install torch torchvision\n",
        "!pip install numpy\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 6.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.6.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeCCtgxDBiGi",
        "colab_type": "text"
      },
      "source": [
        "# Nuestro Script"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBaH0bgAv1KR",
        "colab_type": "text"
      },
      "source": [
        "En google colab, para importar modulos se hace de manera diferente, ya que los notebooks que crea se guardan en una ruta temporal para despues guardarse en drive, por lo que si quieres cargar modulos externos se tiene que hacer:  \n",
        "**import sys**   \n",
        "**sys.path.append('path-carpeta')**  *#esto añade como una caché de rutas*  \n",
        "*from module import function or class*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMipgf50VLnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/ia-course/tema3/libs/')\n",
        "sys.path.append('/content/ia-course/tema3/utils/')\n",
        "sys.path.append('/content/ia-course/tema3/environments/')\n",
        "\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import random\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from datetime import datetime\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from perceptron import SLP\n",
        "from cnn import CNN\n",
        "\n",
        "from decay_schedule import LinearDecaySchedule\n",
        "from experience_memory import ExperienceMemory, Experience\n",
        "from params_manager_colab import ParamsManager\n",
        "\n",
        "import atari as Atari\n",
        "import utils as env_utils\n",
        "\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75d5N1gLJnED",
        "colab_type": "text"
      },
      "source": [
        "# Parseador de Argumentos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tptnugixJpzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = ArgumentParser(\"DeepQLearning\")\n",
        "args.add_argument(\"--params-file\", help = \"Path del fichero JSON de parámetros. El valor por defecto es parameters.json\",\n",
        "                  default=\"/content/ia-course/tema3/parameters.json\", metavar = \"PFILE\")\n",
        "args.add_argument(\"--env\", help = \"Entorno de ID de Atari disponible en OpenAI Gym. El valor por defecto será SeaquestNoFrameskip-v4\",\n",
        "                  default = \"SeaquestNoFrameskip-v4\", metavar=\"ENV\")\n",
        "args.add_argument(\"--gpu-id\", help = \"ID de la GPU a utilizar, por defecto 0\", default = 0, type = int, metavar = \"GPU_ID\")\n",
        "args.add_argument(\"--test\", help = \"Modo de testing para jugar sin aprender. Por defecto está desactivado\", \n",
        "                  action = \"store_true\", default = False)\n",
        "args.add_argument(\"--render\", help = \"Renderiza el entorno en pantalla. Desactivado por defecto\", action=\"store_true\", default=False)\n",
        "args.add_argument(\"--record\", help = \"Almacena videos y estados de la performance del agente\", action=\"store_true\", default=False)\n",
        "args.add_argument(\"--output-dir\", help = \"Directorio para almacenar los outputs. Por defecto = ./trained_models/results\",\n",
        "                  default = \"./trained_models/results\")\n",
        "args, _ = args.parse_known_args()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MewlOwwLJ1Ot",
        "colab_type": "text"
      },
      "source": [
        "# Parámetros globales"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOKy9uUWJ2WX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "manager = ParamsManager(args.params_file)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcW__8l6J7pA",
        "colab_type": "text"
      },
      "source": [
        "# Ficheros de logs acerca de la configuración de las ejecuciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qsMEjXSJ82U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summary_filename_prefix = manager.get_agent_params()['summary_filename_prefix']\n",
        "summary_filename = summary_filename_prefix + args.env + datetime.now().strftime(\"%y-%m-%d-%H-%M\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0b4a6a_FzMO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Summary Writer de TensorBoardX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MIWMd8GF1tL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "writer = SummaryWriter(summary_filename)\n",
        "\n",
        "manager.export_agent_params(summary_filename + \"/\"+\"agent_params.json\")\n",
        "manager.export_environment_params(summary_filename + \"/\"+\"environment_params.json\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VnM12lYF6Tk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#Contador global de ejecuciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVbbs8v7F8JR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global_step_num = 0"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KshEAhVWF_YJ",
        "colab_type": "text"
      },
      "source": [
        "# Habilitar entrenamiento por gráfica o CPU\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEbloYS7F-tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = manager.get_agent_params()['use_cuda']\n",
        "device = torch.device(\"cuda:\"+str(args.gpu_id) if torch.cuda.is_available() and use_cuda else \"cpu\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyhWLSs4GEBL",
        "colab_type": "text"
      },
      "source": [
        "# Habilitar la semilla aleatoria para poder reproducir el experimento a posteriori"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPG2SQKIEhs_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = manager.get_agent_params()['seed']\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "if torch.cuda.is_available() and use_cuda:\n",
        "    torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnBymhWkEau6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeepQLearner(object):\n",
        "    def __init__(self, obs_shape, action_shape, params):\n",
        "       \n",
        "        self.params = params\n",
        "        self.gamma = self.params['gamma']\n",
        "        self.learning_rate = self.params['learning_rate']\n",
        "        self.best_mean_reward = -float(\"inf\")\n",
        "        self.best_reward = -float(\"inf\")\n",
        "        self.training_steps_completed = 0\n",
        "        self.action_shape = action_shape\n",
        "        \n",
        "        if len(obs_shape)  == 1: ## Solo tenemos una dimensión del espacio de observaciones\n",
        "            self.DQN = SLP\n",
        "        elif len(obs_shape) == 3: ## El estado de observaciones es una imagen/3D\n",
        "            self.DQN = CNN\n",
        "            \n",
        "        self.Q = self.DQN(obs_shape, action_shape, device).to(device)\n",
        "        self.Q_optimizer = torch.optim.Adam(self.Q.parameters(), lr = self.learning_rate)\n",
        "        \n",
        "        if self.params['use_target_network']:\n",
        "            self.Q_target = self.DQN(obs_shape, action_shape, device).to(device)\n",
        "           \n",
        "        self.policy = self.epsilon_greedy_Q\n",
        "        self.epsilon_max = self.params['epsilon_max']\n",
        "        self.epsilon_min = self.params['epsilon_min']\n",
        "        self.epsilon_decay = LinearDecaySchedule(initial_value = self.epsilon_max,\n",
        "                                                 final_value = self.epsilon_min, \n",
        "                                                 max_steps = self.params['epsilon_decay_final_step'])\n",
        "        self.step_num = 0\n",
        "        \n",
        "        self.memory = ExperienceMemory(capacity = int(self.params['experience_memory_size']))\n",
        "        \n",
        "         \n",
        "    def get_action(self, obs):\n",
        "        obs = np.array(obs)\n",
        "        obs = obs / 255.0\n",
        "        if len(obs.shape) == 3: # tenemos una imagen\n",
        "            if obs.shape[2] < obs.shape[0]: # WxHxC -> C x H x W\n",
        "                obs = obs.reshape(obs.shape[2], obs.shape[1], obs.shape[0])\n",
        "            obs = np.expand_dims(obs, 0)   \n",
        "        return self.policy(obs)\n",
        "    \n",
        "    def epsilon_greedy_Q(self, obs):\n",
        "        writer.add_scalar(\"DQL/epsilon\", self.epsilon_decay(self.step_num), self.step_num)\n",
        "        self.step_num +=1\n",
        "        if random.random() < self.epsilon_decay(self.step_num) and not self.params[\"test\"]:\n",
        "            action = random.choice([a for a in range(self.action_shape)])\n",
        "        else:\n",
        "            action = np.argmax(self.Q(obs).data.to(torch.device('cpu')).numpy())   \n",
        "        return action\n",
        "        \n",
        "        \n",
        "    def learn(self, obs, action, reward, next_obs, done):\n",
        "        if done:\n",
        "            td_target = reward + 0.0\n",
        "        else: \n",
        "            td_target = reward + self.gamma * torch.max(self.Q(next_obs))\n",
        "        td_error = torch.nn.functional.mse_loss(self.Q(obs)[action], td_target)\n",
        "        self.Q_optimizer.zero_grad()\n",
        "        td_error.backward()\n",
        "        writer.add_scalar(\"DQL/td_error\", td_error.mean(), self.step_num)\n",
        "        self.Q_optimizer.step()\n",
        "        \n",
        "    def replay_experience(self, batch_size = None):\n",
        "        \"\"\"\n",
        "        Vuelve a jugar usando la experiencia aleatoria almacenada\n",
        "        :param batch_size: Tamaño de la muestra a tomar de la memoria\n",
        "        :return: \n",
        "        \"\"\"\n",
        "        batch_size = batch_size if batch_size is not None else self.params['replay_batch_size']\n",
        "        experience_batch = self.memory.sample(batch_size)\n",
        "        self.learn_from_batch_experience(experience_batch)   \n",
        "        self.training_steps_completed += 1\n",
        "      \n",
        "    def learn_from_batch_experience(self, experiences):\n",
        "        \"\"\"\n",
        "        Actualiza la red neuronal profunda en base a lo aprendido en el conjunto de experiencias anteriores\n",
        "        :param experiences: fragmento de recuerdos anteriores\n",
        "        :return: \n",
        "        \"\"\"\n",
        "        batch_xp = Experience(*zip(*experiences))\n",
        "        obs_batch = np.array(batch_xp.obs)/255.0\n",
        "        action_batch = np.array(batch_xp.action)\n",
        "        reward_batch = np.array(batch_xp.reward)\n",
        "        \n",
        "        if self.params[\"clip_reward\"]:\n",
        "            reward_batch = np.sign(reward_batch)\n",
        "        next_obs_batch = np.array(batch_xp.next_obs)/255.0\n",
        "        done_batch = np.array(batch_xp.done)\n",
        "        \n",
        "        \n",
        "        if self.params['use_target_network']:\n",
        "            if self.step_num % self.params['target_network_update_frequency'] == 0:\n",
        "                self.Q_target.load_state_dict(self.Q.state_dict())\n",
        "            td_target = reward_batch + ~done_batch *\\\n",
        "                        np.tile(self.gamma, len(next_obs_batch)) * \\\n",
        "                        torch.max(self.Q_target(next_obs_batch),1)[0].data.tolist()\n",
        "            td_target = torch.from_numpy(td_target)\n",
        "\n",
        "        else: \n",
        "            td_target = reward_batch + ~done_batch * \\\n",
        "                        np.tile(self.gamma, len(next_obs_batch)) * \\\n",
        "                        torch.max(self.Q(next_obs_batch).detach(),1)[0].data.tolist()\n",
        "            td_target = torch.from_numpy(td_target)\n",
        "\n",
        "        \n",
        "        td_target = td_target.to(device)\n",
        "        action_idx = torch.from_numpy(action_batch).to(device)\n",
        "        td_error = torch.nn.functional.mse_loss(\n",
        "                self.Q(obs_batch).gather(1, action_idx.view(-1,1)),\n",
        "                td_target.float().unsqueeze(1))\n",
        "        \n",
        "        self.Q_optimizer.zero_grad()\n",
        "        td_error.mean().backward()\n",
        "        self.Q_optimizer.step()\n",
        "        \n",
        "    def save(self, env_name):\n",
        "        model_save_name = 'model.pt'\n",
        "        path = F\"/content/drive/My Drive/{model_save_name}\" \n",
        "        file_name = self.params['save_dir']+\"DQL_\"+env_name+\".ptm\"\n",
        "        agent_state = {\"Q\": self.Q.state_dict(),\n",
        "                       \"best_mean_reward\": self.best_mean_reward,\n",
        "                       \"best_reward\": self.best_reward}\n",
        "        torch.save(agent_state, file_name)\n",
        "        print(\"Estado del agente guardado en : \", file_name)\n",
        "        \n",
        "        \n",
        "    def load(self, env_name):\n",
        "        path = F\"/content/drive/My Drive/trained_models/model.pt\"\n",
        "        file_name = self.params['load_dir']+\"DQL_\"+env_name+\".ptm\"\n",
        "        agent_state = torch.load(file_name, map_location = lambda storage, loc: storage)\n",
        "        self.Q.load_state_dict(agent_state[\"Q\"])\n",
        "        self.Q.to(device)\n",
        "        self.best_mean_reward = agent_state[\"best_mean_reward\"]\n",
        "        self.best_reward = agent_state[\"best_reward\"]\n",
        "        print(\"Cargado del modelo Q desde\", file_name,\n",
        "              \"que hasta el momento tiene una mejor recompensa media de: \",self.best_mean_reward,\n",
        "              \" y una recompensa máxima de: \", self.best_reward)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrgbOwhvuI5_",
        "colab_type": "text"
      },
      "source": [
        "# Activar TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJfOTtl1uLl1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnrgnMAGtUBk",
        "colab_type": "text"
      },
      "source": [
        "# TensorBoardX Juan Gabriel Gomila\n",
        "#### Si no funciona vuelve a ejecutar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-2aV-AztYH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40607734-b783-4cf8-a084-f1f1ec23f865"
      },
      "source": [
        "%tensorboard --logdir /content/ia-course/tema3/logs"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 1580."
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg5F6OERzRvD",
        "colab_type": "text"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d39qUlVjENDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3bb443a6-8bc9-4ef9-8a85-7294d1445d10"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    env_conf = manager.get_environment_params()\n",
        "    env_conf[\"env_name\"] = args.env\n",
        "    \n",
        "    if args.test:\n",
        "        env_conf[\"episodic_life\"] = False\n",
        "    reward_type = \"LIFE\" if env_conf[\"episodic_life\"] else \"GAME\"\n",
        "    \n",
        "    custom_region_available = False\n",
        "    for key, value in env_conf[\"useful_region\"].items():\n",
        "        if key in args.env:\n",
        "            env_conf[\"useful_region\"] = value\n",
        "            custom_region_available = True\n",
        "            break\n",
        "    if custom_region_available is not True:\n",
        "        env_conf[\"useful_region\"] = env_conf[\"useful_region\"][\"Default\"]\n",
        "    print(\"Configuración a utilizar:\", env_conf)\n",
        "    \n",
        "    atari_env = False\n",
        "    for game in Atari.get_games_list():\n",
        "        if game.replace(\"_\", \"\") in args.env.lower():\n",
        "            atari_env = True\n",
        "    \n",
        "    if atari_env:\n",
        "        environment = Atari.make_env(args.env, env_conf)\n",
        "    else:\n",
        "        environment = env_utils.ResizeReshapeFrames(gym.make(args.env))\n",
        "        \n",
        "    obs_shape = environment.observation_space.shape\n",
        "    action_shape = environment.action_space.n\n",
        "    agent_params = manager.get_agent_params()\n",
        "    agent_params[\"test\"] = args.test\n",
        "    agent_params[\"clip_reward\"] = env_conf[\"clip_reward\"]\n",
        "    agent = DeepQLearner(obs_shape, action_shape, agent_params)\n",
        "    \n",
        "    episode_rewards = list()\n",
        "    previous_checkpoint_mean_ep_rew = agent.best_mean_reward\n",
        "    num_improved_episodes_before_checkpoint = 0\n",
        "    if agent_params['load_trained_model']:\n",
        "        try:\n",
        "            agent.load(env_conf['env_name'])\n",
        "            previous_checkpoint_mean_ep_rew = agent.best_mean_reward\n",
        "        except FileNotFoundError:\n",
        "            print(\"ERROR: no existe ningún modelo entrenado para este entorno. Empezamos desde cero\")\n",
        "\n",
        "    \n",
        "    episode = 0\n",
        "    while global_step_num < agent_params['max_training_steps']:\n",
        "        obs = environment.reset()\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        step = 0\n",
        "        while not done: \n",
        "            if env_conf['render'] or args.render:\n",
        "                environment.render()\n",
        "            \n",
        "            action = agent.get_action(obs)\n",
        "            next_obs, reward, done, info = environment.step(action)\n",
        "            agent.memory.store(Experience(obs, action, reward, next_obs, done))\n",
        "            \n",
        "            obs = next_obs\n",
        "            total_reward += reward\n",
        "            step += 1\n",
        "            global_step_num += 1\n",
        "            \n",
        "            if done is True:\n",
        "                episode += 1\n",
        "                episode_rewards.append(total_reward)\n",
        "            \n",
        "                if total_reward > agent.best_reward:\n",
        "                    agent.best_reward = total_reward\n",
        "                \n",
        "                if np.mean(episode_rewards) > previous_checkpoint_mean_ep_rew: \n",
        "                    num_improved_episodes_before_checkpoint += 1\n",
        "                \n",
        "                if num_improved_episodes_before_checkpoint >= agent_params['save_freq']:\n",
        "                    previous_checkpoint_mean_ep_rew = np.mean(episode_rewards)\n",
        "                    agent.best_mean_reward = np.mean(episode_rewards)\n",
        "                    agent.save(env_conf['env_name'])\n",
        "                    num_improved_episodes_before_checkpoint = 0\n",
        "                \n",
        "                print(\"\\n Episodio #{} finalizado con {} iteraciones. Con {} estados: recompensa = {}, recompensa media = {:.2f}, mejor recompensa = {}\".\n",
        "                      format(episode, step+1, reward_type, total_reward, np.mean(episode_rewards), agent.best_reward))\n",
        "                \n",
        "                writer.add_scalar(\"main/ep_reward\", total_reward, global_step_num)\n",
        "                writer.add_scalar(\"main/mean_ep_reward\", np.mean(episode_rewards), global_step_num)\n",
        "                writer.add_scalar(\"main/max_ep_reward\", agent.best_reward, global_step_num)\n",
        "                \n",
        "                if agent.memory.get_size() >= 2*agent_params['replay_start_size'] and not args.test:\n",
        "                    agent.replay_experience()\n",
        "                    \n",
        "                break\n",
        "            \n",
        "    environment.close()\n",
        "    writer.close()\n",
        "    show_videos()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Configuración a utilizar: {'type': 'Atari', 'episodic_life': True, 'clip_reward': True, 'skip_rate': 4, 'num_frames_to_stack': 4, 'render': False, 'normalize_observation': False, 'useful_region': {'crop1': 30, 'crop2': 30, 'dimension2': 80}, 'env_name': 'SeaquestNoFrameskip-v4'}\n",
            "ERROR: no existe ningún modelo entrenado para este entorno. Empezamos desde cero\n",
            "\n",
            " Episodio #1 finalizado con 130 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 3.00, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #2 finalizado con 125 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 2.00, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #3 finalizado con 83 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.33, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #4 finalizado con 147 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.50, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #5 finalizado con 208 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.80, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #6 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.50, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #7 finalizado con 130 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.43, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #8 finalizado con 147 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.38, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #9 finalizado con 104 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.44, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #10 finalizado con 85 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.30, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #11 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.18, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #12 finalizado con 149 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.17, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #13 finalizado con 148 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.23, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #14 finalizado con 100 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.14, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #15 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #16 finalizado con 62 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #17 finalizado con 122 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.00, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #18 finalizado con 89 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.94, mejor recompensa = 3.0\n",
            "\n",
            " Episodio #19 finalizado con 343 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.21, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #20 finalizado con 225 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.35, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #21 finalizado con 93 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.29, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #22 finalizado con 98 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.23, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #23 finalizado con 275 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.30, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #24 finalizado con 140 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.29, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #25 finalizado con 193 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.32, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #26 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.27, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #27 finalizado con 113 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.26, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #28 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.21, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #29 finalizado con 142 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.21, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #30 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.17, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #31 finalizado con 160 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.19, mejor recompensa = 6.0\n",
            "\n",
            " Episodio #32 finalizado con 406 iteraciones. Con LIFE estados: recompensa = 9.0, recompensa media = 1.44, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #33 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.39, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #34 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.38, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #35 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.34, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #36 finalizado con 295 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 1.44, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #37 finalizado con 104 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.41, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #38 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.37, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #39 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.36, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #40 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.32, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #41 finalizado con 109 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.32, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #42 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.29, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #43 finalizado con 64 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.26, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #44 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.23, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #45 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.20, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #46 finalizado con 104 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.17, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #47 finalizado con 92 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.15, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #48 finalizado con 127 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.15, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #49 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "Estado del agente guardado en :  /content/drive/My Drive/DQL_SeaquestNoFrameskip-v4.ptm\n",
            "\n",
            " Episodio #50 finalizado con 354 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #51 finalizado con 200 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.18, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #52 finalizado con 141 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.17, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #53 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.15, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #54 finalizado con 114 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.15, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #55 finalizado con 147 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #56 finalizado con 374 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #57 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #58 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #59 finalizado con 77 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #60 finalizado con 227 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #61 finalizado con 158 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #62 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #63 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #64 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #65 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #66 finalizado con 167 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #67 finalizado con 234 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #68 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #69 finalizado con 140 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #70 finalizado con 82 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #71 finalizado con 252 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #72 finalizado con 134 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #73 finalizado con 106 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #74 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #75 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #76 finalizado con 123 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #77 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #78 finalizado con 122 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #79 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #80 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #81 finalizado con 96 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #82 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #83 finalizado con 277 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.05, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #84 finalizado con 173 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #85 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #86 finalizado con 290 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #87 finalizado con 100 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #88 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #89 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #90 finalizado con 61 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #91 finalizado con 85 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #92 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #93 finalizado con 197 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.04, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #94 finalizado con 165 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.04, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #95 finalizado con 126 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.05, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #96 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #97 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #98 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.03, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #99 finalizado con 145 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #100 finalizado con 108 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #101 finalizado con 123 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #102 finalizado con 150 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #103 finalizado con 108 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #104 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #105 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.98, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #106 finalizado con 320 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.00, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #107 finalizado con 116 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #108 finalizado con 386 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #109 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #110 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #111 finalizado con 58 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #112 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #113 finalizado con 117 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #114 finalizado con 147 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #115 finalizado con 127 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #116 finalizado con 141 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #117 finalizado con 109 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #118 finalizado con 412 iteraciones. Con LIFE estados: recompensa = 7.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #119 finalizado con 143 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #120 finalizado con 87 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #121 finalizado con 334 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #122 finalizado con 155 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #123 finalizado con 61 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #124 finalizado con 438 iteraciones. Con LIFE estados: recompensa = 7.0, recompensa media = 1.15, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #125 finalizado con 107 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #126 finalizado con 123 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #127 finalizado con 77 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #128 finalizado con 108 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #129 finalizado con 62 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #130 finalizado con 123 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #131 finalizado con 146 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #132 finalizado con 80 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #133 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #134 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #135 finalizado con 95 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #136 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #137 finalizado con 80 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #138 finalizado con 129 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #139 finalizado con 295 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #140 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #141 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #142 finalizado con 241 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #143 finalizado con 59 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #144 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #145 finalizado con 409 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #146 finalizado con 136 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #147 finalizado con 184 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #148 finalizado con 123 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #149 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #150 finalizado con 117 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #151 finalizado con 203 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #152 finalizado con 126 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #153 finalizado con 323 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #154 finalizado con 108 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #155 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #156 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #157 finalizado con 99 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #158 finalizado con 89 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #159 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #160 finalizado con 85 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #161 finalizado con 279 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #162 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #163 finalizado con 98 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #164 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #165 finalizado con 57 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #166 finalizado con 87 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #167 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #168 finalizado con 113 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #169 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #170 finalizado con 117 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #171 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #172 finalizado con 125 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #173 finalizado con 85 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #174 finalizado con 249 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #175 finalizado con 263 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #176 finalizado con 141 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #177 finalizado con 97 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #178 finalizado con 553 iteraciones. Con LIFE estados: recompensa = 8.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #179 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #180 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #181 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #182 finalizado con 157 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #183 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #184 finalizado con 122 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #185 finalizado con 247 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #186 finalizado con 126 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #187 finalizado con 141 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #188 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #189 finalizado con 258 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #190 finalizado con 58 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #191 finalizado con 137 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #192 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #193 finalizado con 212 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.15, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #194 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #195 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #196 finalizado con 134 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #197 finalizado con 92 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #198 finalizado con 114 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #199 finalizado con 132 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #200 finalizado con 174 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #201 finalizado con 127 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #202 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #203 finalizado con 67 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #204 finalizado con 127 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #205 finalizado con 118 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #206 finalizado con 233 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #207 finalizado con 90 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #208 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #209 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #210 finalizado con 295 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #211 finalizado con 83 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #212 finalizado con 176 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #213 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #214 finalizado con 129 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #215 finalizado con 287 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #216 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #217 finalizado con 190 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #218 finalizado con 147 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.14, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #219 finalizado con 92 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #220 finalizado con 102 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #221 finalizado con 146 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #222 finalizado con 98 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #223 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #224 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #225 finalizado con 91 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #226 finalizado con 91 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #227 finalizado con 100 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #228 finalizado con 139 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #229 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #230 finalizado con 122 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #231 finalizado con 136 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #232 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #233 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #234 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #235 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #236 finalizado con 87 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #237 finalizado con 121 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #238 finalizado con 168 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #239 finalizado con 117 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #240 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #241 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #242 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #243 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #244 finalizado con 270 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #245 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #246 finalizado con 116 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #247 finalizado con 85 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #248 finalizado con 147 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #249 finalizado con 144 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #250 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #251 finalizado con 73 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #252 finalizado con 317 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #253 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #254 finalizado con 119 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #255 finalizado con 201 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #256 finalizado con 108 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #257 finalizado con 139 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #258 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #259 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #260 finalizado con 229 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #261 finalizado con 64 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #262 finalizado con 112 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #263 finalizado con 60 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #264 finalizado con 281 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #265 finalizado con 264 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #266 finalizado con 109 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #267 finalizado con 82 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #268 finalizado con 178 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #269 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #270 finalizado con 164 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #271 finalizado con 226 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #272 finalizado con 569 iteraciones. Con LIFE estados: recompensa = 9.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #273 finalizado con 170 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #274 finalizado con 70 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #275 finalizado con 66 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #276 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #277 finalizado con 111 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #278 finalizado con 172 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #279 finalizado con 206 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #280 finalizado con 204 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #281 finalizado con 106 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #282 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #283 finalizado con 173 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #284 finalizado con 85 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #285 finalizado con 124 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #286 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #287 finalizado con 118 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #288 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #289 finalizado con 97 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #290 finalizado con 79 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #291 finalizado con 59 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #292 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #293 finalizado con 115 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #294 finalizado con 121 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #295 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #296 finalizado con 172 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #297 finalizado con 96 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #298 finalizado con 188 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #299 finalizado con 120 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #300 finalizado con 105 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #301 finalizado con 132 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #302 finalizado con 213 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #303 finalizado con 123 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #304 finalizado con 416 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #305 finalizado con 139 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #306 finalizado con 88 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #307 finalizado con 153 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #308 finalizado con 339 iteraciones. Con LIFE estados: recompensa = 6.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #309 finalizado con 266 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #310 finalizado con 106 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #311 finalizado con 97 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #312 finalizado con 142 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #313 finalizado con 138 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #314 finalizado con 117 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #315 finalizado con 169 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #316 finalizado con 161 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #317 finalizado con 121 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #318 finalizado con 181 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #319 finalizado con 174 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #320 finalizado con 83 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #321 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #322 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #323 finalizado con 354 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #324 finalizado con 264 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #325 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #326 finalizado con 148 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.13, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #327 finalizado con 93 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #328 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #329 finalizado con 65 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #330 finalizado con 140 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.12, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #331 finalizado con 74 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #332 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #333 finalizado con 102 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #334 finalizado con 149 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.11, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #335 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #336 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #337 finalizado con 126 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #338 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #339 finalizado con 75 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #340 finalizado con 96 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #341 finalizado con 106 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #342 finalizado con 218 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #343 finalizado con 156 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #344 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #345 finalizado con 76 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #346 finalizado con 194 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.10, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #347 finalizado con 91 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #348 finalizado con 74 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #349 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #350 finalizado con 118 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #351 finalizado con 131 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #352 finalizado con 125 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.09, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #353 finalizado con 123 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #354 finalizado con 69 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #355 finalizado con 109 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #356 finalizado con 84 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #357 finalizado con 93 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #358 finalizado con 145 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.08, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #359 finalizado con 126 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #360 finalizado con 123 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #361 finalizado con 92 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #362 finalizado con 91 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #363 finalizado con 166 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.07, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #364 finalizado con 92 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #365 finalizado con 64 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #366 finalizado con 94 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.06, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #367 finalizado con 80 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #368 finalizado con 61 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #369 finalizado con 59 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #370 finalizado con 87 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.05, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #371 finalizado con 149 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.05, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #372 finalizado con 148 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.05, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #373 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #374 finalizado con 71 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #375 finalizado con 98 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.04, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #376 finalizado con 63 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #377 finalizado con 90 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #378 finalizado con 93 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #379 finalizado con 80 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.03, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #380 finalizado con 135 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.03, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #381 finalizado con 58 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #382 finalizado con 103 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.02, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #383 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #384 finalizado con 114 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #385 finalizado con 68 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.02, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #386 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #387 finalizado con 78 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #388 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #389 finalizado con 96 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #390 finalizado con 81 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #391 finalizado con 60 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #392 finalizado con 80 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #393 finalizado con 105 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #394 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #395 finalizado con 58 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #396 finalizado con 161 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 0.99, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #397 finalizado con 170 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 0.99, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #398 finalizado con 334 iteraciones. Con LIFE estados: recompensa = 3.0, recompensa media = 1.00, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #399 finalizado con 97 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.00, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #400 finalizado con 72 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #401 finalizado con 64 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 0.99, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #402 finalizado con 173 iteraciones. Con LIFE estados: recompensa = 2.0, recompensa media = 1.00, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #403 finalizado con 284 iteraciones. Con LIFE estados: recompensa = 5.0, recompensa media = 1.00, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #404 finalizado con 145 iteraciones. Con LIFE estados: recompensa = 4.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #405 finalizado con 86 iteraciones. Con LIFE estados: recompensa = 0.0, recompensa media = 1.01, mejor recompensa = 9.0\n",
            "\n",
            " Episodio #406 finalizado con 128 iteraciones. Con LIFE estados: recompensa = 1.0, recompensa media = 1.01, mejor recompensa = 9.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWLeaKfCtMGb",
        "colab_type": "text"
      },
      "source": [
        "# TensorBoardX Ejecucion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaUT9mszEgT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir /content/logs/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}